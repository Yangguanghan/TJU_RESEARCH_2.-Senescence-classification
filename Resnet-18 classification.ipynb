{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d00a1a",
   "metadata": {},
   "source": [
    "Resnet-18 deep learning based on three classes of senescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6654b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, glob, random, cv2, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tifffile, matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# ----------------------------- Configuration -----------------------------\n",
    "data_dirs = {\n",
    "    \"Healthy\":   r\"E:\\0. Research\\1. Paper_data\\2. Hacat cisplatin ROS\\数据集\\原始数据\\downsample-healthy\",\n",
    "    \"Cisplatin\": r\"E:\\0. Research\\1. Paper_data\\2. Hacat cisplatin ROS\\数据集\\原始数据\\Cisplatin\",\n",
    "    \"H2O2\":      r\"E:\\0. Research\\1. Paper_data\\2. Hacat cisplatin ROS\\数据集\\原始数据\\H2O2\"\n",
    "}\n",
    "batch_size   = 32\n",
    "resize_size  = 128\n",
    "num_epochs   = 150\n",
    "num_seeds    = 3\n",
    "learning_rate= 1e-3\n",
    "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "output_dir   = r\"E:\\2. Senescence\\CNN模型训练结果\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"Device:\", device, torch.cuda.get_device_name(0) if device.type=='cuda' else \"\")\n",
    "\n",
    "# ----------------------------- Dataset -----------------------------\n",
    "class PSData(Dataset):\n",
    "    def __init__(self, data_dirs, resize=128, stats=None):\n",
    "        self.samples, self.labels = [], []\n",
    "        self.resize = resize\n",
    "        self.stats  = stats\n",
    "        label_map = {k: i for i, k in enumerate(data_dirs)}\n",
    "        for name, folder in data_dirs.items():\n",
    "            for p in sorted(glob.glob(os.path.join(folder, \"*-P*.tif*\"))):\n",
    "                s = p.replace(\"-P\", \"-S\")\n",
    "                if os.path.exists(s):\n",
    "                    self.samples.append((p, s))\n",
    "                    self.labels.append(label_map[name])\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p, s = self.samples[idx]\n",
    "        p_img = cv2.resize(tifffile.imread(p).astype(np.float32)/65535., (self.resize, self.resize))\n",
    "        s_img = cv2.resize(tifffile.imread(s).astype(np.float32)/65535., (self.resize, self.resize))\n",
    "        img   = np.stack([p_img, s_img], 0)\n",
    "        if self.stats is not None:\n",
    "            p_mean = self.stats['p_mean'].cpu().numpy()\n",
    "            p_std  = self.stats['p_std'].cpu().numpy()\n",
    "            s_mean = self.stats['s_mean'].cpu().numpy()\n",
    "            s_std  = self.stats['s_std'].cpu().numpy()\n",
    "            img[0] = (img[0] - p_mean) / p_std\n",
    "            img[1] = (img[1] - s_mean) / s_std\n",
    "        return torch.tensor(img, dtype=torch.float32), self.labels[idx]\n",
    "\n",
    "# ----------------------------- Compute training set statistics -----------------------------\n",
    "@torch.no_grad()\n",
    "def compute_stats(train_loader):\n",
    "    p_sum = p_sq = s_sum = s_sq = cnt = 0\n",
    "    for img, _ in train_loader:\n",
    "        p, s = img[:,0], img[:,1]\n",
    "        p_sum += p.sum(); p_sq += (p**2).sum()\n",
    "        s_sum += s.sum(); s_sq += (s**2).sum()\n",
    "        cnt   += p.numel()\n",
    "    p_mean, s_mean = p_sum/cnt, s_sum/cnt\n",
    "    p_std  = torch.sqrt(p_sq/cnt - p_mean**2).clamp(min=1e-6)\n",
    "    s_std  = torch.sqrt(s_sq/cnt - s_mean**2).clamp(min=1e-6)\n",
    "    return {'p_mean':p_mean,'p_std':p_std,'s_mean':s_mean,'s_std':s_std}\n",
    "\n",
    "# ----------------------------- Network -----------------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Use 'weights' argument instead of 'pretrained' to avoid warnings\n",
    "        backbone = resnet18(weights=None)\n",
    "        backbone.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        backbone.fc = nn.Linear(backbone.fc.in_features, num_classes)\n",
    "        self.net = backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------------- Training -----------------------------\n",
    "def train_one(seed):\n",
    "    torch.manual_seed(seed); random.seed(seed); np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    full_dataset = PSData(data_dirs, resize_size, stats=None)\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = int(0.1 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_idx, val_idx, test_idx = random_split(\n",
    "        range(total_size),\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    train_dataset_raw = Subset(full_dataset, train_idx)\n",
    "    train_loader_raw = DataLoader(train_dataset_raw, batch_size, shuffle=False, num_workers=0)\n",
    "    train_stats = compute_stats(train_loader_raw)\n",
    "    torch.save(train_stats, os.path.join(output_dir, f'train_stats_seed{seed}.pt'))\n",
    "\n",
    "    full_dataset_normalized = PSData(data_dirs, resize_size, stats=train_stats)\n",
    "    train_dataset = Subset(full_dataset_normalized, train_idx)\n",
    "    val_dataset = Subset(full_dataset_normalized, val_idx)\n",
    "    test_dataset = Subset(full_dataset_normalized, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = SimpleCNN(len(data_dirs)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=device.type=='cuda')\n",
    "\n",
    "    history = {\n",
    "        \"train_acc\": [], \"val_acc\": [], \"test_acc\": [],\n",
    "        \"train_loss\": [], \"val_loss\": [], \"test_loss\": []\n",
    "    }\n",
    "    preds_final, labels_final = [], []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=f\"Seed {seed}\"):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_correct = 0; train_total = 0; train_loss = 0.0\n",
    "        for img, lab in train_loader:\n",
    "            img, lab = img.to(device, non_blocking=True), lab.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=device.type=='cuda'):\n",
    "                outputs = model(img)\n",
    "                loss = criterion(outputs, lab)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_correct += (outputs.argmax(1) == lab).sum().item()\n",
    "            train_total += lab.size(0)\n",
    "            train_loss += loss.item() * lab.size(0)\n",
    "\n",
    "        history[\"train_acc\"].append(train_correct / train_total)\n",
    "        history[\"train_loss\"].append(train_loss / train_total)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct = 0; val_total = 0; val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for img, lab in val_loader:\n",
    "                img, lab = img.to(device, non_blocking=True), lab.to(device, non_blocking=True)\n",
    "                outputs = model(img)\n",
    "                loss = criterion(outputs, lab)\n",
    "\n",
    "                val_correct += (outputs.argmax(1) == lab).sum().item()\n",
    "                val_total += lab.size(0)\n",
    "                val_loss += loss.item() * lab.size(0)\n",
    "\n",
    "        history[\"val_acc\"].append(val_correct / val_total)\n",
    "        history[\"val_loss\"].append(val_loss / val_total)\n",
    "\n",
    "        # Testing phase\n",
    "        test_correct = 0; test_total = 0; test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for img, lab in test_loader:\n",
    "                img, lab = img.to(device, non_blocking=True), lab.to(device, non_blocking=True)\n",
    "                outputs = model(img)\n",
    "                loss = criterion(outputs, lab)\n",
    "\n",
    "                test_correct += (outputs.argmax(1) == lab).sum().item()\n",
    "                test_total += lab.size(0)\n",
    "                test_loss += loss.item() * lab.size(0)\n",
    "\n",
    "                if epoch == num_epochs - 1:\n",
    "                    preds_final.extend(outputs.argmax(1).cpu().numpy())\n",
    "                    labels_final.extend(lab.cpu().numpy())\n",
    "\n",
    "        history[\"test_acc\"].append(test_correct / test_total)\n",
    "        history[\"test_loss\"].append(test_loss / test_total)\n",
    "\n",
    "    # Output classification report\n",
    "    print(f\"\\n=== Classification Report (Seed {seed}) ===\")\n",
    "    print(classification_report(\n",
    "        labels_final, preds_final,\n",
    "        target_names=list(data_dirs.keys()),\n",
    "        digits=4\n",
    "    ))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(labels_final, preds_final)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d',\n",
    "        xticklabels=list(data_dirs.keys()),\n",
    "        yticklabels=list(data_dirs.keys()),\n",
    "        cmap='Blues', cbar=False\n",
    "    )\n",
    "    plt.title(f\"Confusion Matrix (Seed {seed})\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    # Fix: use correct parameter bbox_inches='tight'\n",
    "    plt.savefig(os.path.join(output_dir, f\"cm_seed{seed}.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, f\"model_seed{seed}.pth\"))\n",
    "    torch.save(history, os.path.join(output_dir, f\"history_seed{seed}.pt\"))\n",
    "\n",
    "    return history\n",
    "\n",
    "# ----------------------------- Main process -----------------------------\n",
    "histories = []\n",
    "for seed in range(num_seeds):\n",
    "    history = train_one(seed)\n",
    "    histories.append(history)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------------------- Plot averaged curves -----------------------------\n",
    "def smooth(y, window=5):\n",
    "    return [np.mean(y[max(0, i-window+1):i+1]) for i in range(len(y))]\n",
    "\n",
    "# 1. Accuracy curves\n",
    "keys_acc = [\"train_acc\", \"val_acc\", \"test_acc\"]\n",
    "labels_acc = [\"Train\", \"Validation\", \"Test\"]\n",
    "mean_acc = {k: np.mean([h[k] for h in histories], axis=0) for k in keys_acc}\n",
    "std_acc = {k: np.std([h[k] for h in histories], axis=0) for k in keys_acc}\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for k, lab in zip(keys_acc, labels_acc):\n",
    "    mean_smoothed = smooth(mean_acc[k])\n",
    "    std = std_acc[k]\n",
    "    plt.plot(mean_smoothed, label=lab, linewidth=1.5)\n",
    "    plt.fill_between(\n",
    "        range(len(mean_smoothed)),\n",
    "        mean_smoothed - std,\n",
    "        mean_smoothed + std,\n",
    "        alpha=0.2\n",
    "    )\n",
    "plt.xlabel(\"Epoch\", fontsize=10)\n",
    "plt.ylabel(\"Accuracy\", fontsize=10)\n",
    "plt.legend(fontsize=8)\n",
    "plt.title(\"Accuracy Curve (Mean ± Std)\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "# Fix: use correct parameter bbox_inches='tight'\n",
    "plt.savefig(os.path.join(output_dir, \"accuracy_curve.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Loss curves\n",
    "keys_loss = [\"train_loss\", \"val_loss\", \"test_loss\"]\n",
    "labels_loss = [\"Train\", \"Validation\", \"Test\"]\n",
    "mean_loss = {k: np.mean([h[k] for h in histories], axis=0) for k in keys_loss}\n",
    "std_loss = {k: np.std([h[k] for h in histories], axis=0) for k in keys_loss}\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for k, lab in zip(keys_loss, labels_loss):\n",
    "    mean_smoothed = smooth(mean_loss[k])\n",
    "    std = std_loss[k]\n",
    "    plt.plot(mean_smoothed, label=lab, linewidth=1.5)\n",
    "    plt.fill_between(\n",
    "        range(len(mean_smoothed)),\n",
    "        mean_smoothed - std,\n",
    "        mean_smoothed + std,\n",
    "        alpha=0.2\n",
    "    )\n",
    "plt.xlabel(\"Epoch\", fontsize=10)\n",
    "plt.ylabel(\"Cross-Entropy Loss\", fontsize=10)\n",
    "plt.legend(fontsize=8)\n",
    "plt.title(\"Loss Curve (Mean ± Std)\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "# Fix: use correct parameter bbox_inches='tight'\n",
    "plt.savefig(os.path.join(output_dir, \"loss_curve.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------- Final statistics -----------------------------\n",
    "final_epoch_window = 10\n",
    "print(\"\\n=== Final Average Performance (All Seeds) ===\")\n",
    "for phase in [\"train\", \"val\", \"test\"]:\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    for history in histories:\n",
    "        final_acc = np.mean(history[f\"{phase}_acc\"][-final_epoch_window:])\n",
    "        final_loss = np.mean(history[f\"{phase}_loss\"][-final_epoch_window:])\n",
    "        acc_list.append(final_acc)\n",
    "        loss_list.append(final_loss)\n",
    "    print(f\"{phase.capitalize()} Accuracy: {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "    print(f\"{phase.capitalize()} Loss:     {np.mean(loss_list):.4f} ± {np.std(loss_list):.4f}\")\n",
    "\n",
    "torch.save(histories, os.path.join(output_dir, \"all_seeds_histories.pt\"))\n",
    "print(f\"\\nAll results saved to: {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
